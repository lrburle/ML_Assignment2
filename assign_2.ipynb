{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 - Landon Burleson \n",
    "## Problem 1\n",
    "As described in the assignment document, this problem will discuss the derivation of the update rule\n",
    "using the sigmoid activation function and the Mean Squared loss function. See the following LaTex\n",
    "equations below for a step-by-step guide through the described derivation. \n",
    "\n",
    "The following equation is the Mean Square Error (MSE) Loss function used for this derivation:\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{n} \\sum_{i = 1}^{n}(h_\\theta(x^{(i)}) - y^{(i)})^2$$\n",
    "\n",
    "The sigmoid function used for the activation function within this example is shown below:\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1+e^x}$$\n",
    "\n",
    "The derivative of the sigmoid function is shown here:\n",
    "$$ \\frac{d\\sigma}{dx} = \\frac{e^{-x}}{(1+e^{-x})^2}$$\n",
    "\n",
    "\n",
    "$$\\frac{dW}{d\\theta} = $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding in the necessary modules needed to complete Assignment 2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class assign2:\n",
    "        def __init__(self):\n",
    "                self.a = 0.3 #Learning rate\n",
    "\n",
    "        def init_theta(self, m, n):\n",
    "                self.theta = np.random.random((m, n))\n",
    "\n",
    "        # g(w0 + sum(x*w))\n",
    "        def perceptron(self, x, w, g):\n",
    "                return g(np.dot(x, w))\n",
    "        \n",
    "        def hiddenLayer(self, x, w, number_of_neurons):\n",
    "                y = []\n",
    "                for i in range(number_of_neurons):\n",
    "                        y.append(self.perceptron(x, w))\n",
    "                return y\n",
    "\n",
    "        def backprop(self):\n",
    "                for i in range(25):\n",
    "                        dz2 = (a2 - y)\n",
    "                        dW2 = np.dot(dz2, a1.T)\n",
    "                        db2 = dz2\n",
    "                        dz1 = np.dot(W2.T, dz2) * sigmoid(z1) * (1-sigmoid(z1))\n",
    "                        dW1 = np.dot(dz1, x.T)\n",
    "                        db1 = dz1\n",
    "                        W1 = W1 - dW1\n",
    "                        W2 = W2 - dW2\n",
    "                        b1 = b1 - db1\n",
    "                        b2 = b2 - db2\n",
    "                        z1New = np.dot(W1, x) + b1\n",
    "                        a1New = sigmoid(z1New)\n",
    "                        z2New = np.dot(W2, a1New) + b2\n",
    "                        a2New = sigmoid(z2New)\n",
    "                        print(a2New, a2)\n",
    "\n",
    "        def hypothesis(self, x, w, b):\n",
    "                y = np.dot(x, w) + b\n",
    "                return y\n",
    "\n",
    "        def costFunction(self, xdata, ydata):\n",
    "                [m, n] = xdata.shape\n",
    "\n",
    "                sum = 0\n",
    "                for i in range(m):\n",
    "                        sum += (self.hypothesis(xdata[i])-ydata[i])**2\n",
    "                costf = (1/m) * sum\n",
    "\n",
    "                return costf\n",
    "\n",
    "\t#Used to modifify the input data appropriately\n",
    "        def sigmoid(self, z):\n",
    "                return 1 / (1 + np.exp(-z))\n",
    "\n",
    "        def sigmoidDerivative(self, z):\n",
    "                return self.sigmoid(z) * (1 - self.sigmoid(z))\n",
    "\n",
    "        def gradientDescent(self, xdata, ydata):\n",
    "                [m, n] = xdata.shape\n",
    "\n",
    "                theta_new = self.theta.copy()\n",
    "\n",
    "                for j in range(n):\n",
    "                        sum = 0 \n",
    "                        for i in range(m):\n",
    "                                sum += (self.hypothesis(xdata[i]) - ydata[i]) * xdata[i, j]\n",
    "\n",
    "                        theta_new[0, j] = self.theta[0, j] - self.a * (1 / m) * sum\n",
    "                \n",
    "                self.theta = theta_new \n",
    "\n",
    "        def concatOnes(self, data):\n",
    "                [m,n] = data.shape\n",
    "                out = np.ones((m, 1))\n",
    "                return np.concatenate((out, data), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot load file containing pickled data when allow_pickle=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7447/2380628451.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#Load in the data to be used for question 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'X_test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'X_train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Y_test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0;31m# Try a pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m                 raise ValueError(\"Cannot load file containing pickled data \"\n\u001b[0m\u001b[1;32m    436\u001b[0m                                  \"when allow_pickle=False\")\n\u001b[1;32m    437\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot load file containing pickled data when allow_pickle=False"
     ]
    }
   ],
   "source": [
    "\n",
    "#Main Code\n",
    "a2 = assign2()\n",
    "\n",
    "#Load in the data to be used for question 1\n",
    "x_test = np.load('X_test.csv')\n",
    "x_train = np.load('X_train.csv')\n",
    "y_test = np.load('Y_test.csv')\n",
    "y_train = np.load('Y_train.csv')\n",
    "\n",
    "#Plot the initial training data and hypothesis function.\n",
    "plt.figure(0, figsize=[15, 12])\n",
    "plt.xlabel('x', fontsize=18)\n",
    "plt.ylabel('y', fontsize=18)\n",
    "plt.title(f'Q2 - Data Set - No Basis Functions, ' + r'$\\alpha$ = ' + f'{a1.a}', fontsize=24)\n",
    "plt.ylim(-50, 70)\n",
    "og, hypth = plt.plot(x_train, y_train, 'o')\n",
    "\n",
    "hypth.set_label('Hypothesis')\n",
    "og.set_label('Training Data Set')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid()\n",
    "\n",
    "print(f'Initialzed theta array is: {a1.theta}')\n",
    "\n",
    "iterations = 100\n",
    "for k in range(iterations):\n",
    "\tprint(f'Current iteration is {k} @ error = {error[-1]}')\n",
    "\n",
    "\ta2.gradientDescent(x_train, y_train)\n",
    "\tx, h = a2.hypothesisDataGeneration(np.min(x_train[:, 1]),np.max(x_train[:,1]), order)\n",
    "\terror.append(a2.costFunction(x_train, y_train))\n",
    "\tepsilon = np.abs(error[-1] - error[-2])\n",
    "\thypth.set_ydata(h)\n",
    "\tplt.draw()\n",
    "\tif (epsilon < 10e-6):\n",
    "\t\tprint('Convergence threshold met.')\n",
    "\t\tbreak"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
